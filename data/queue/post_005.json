{
    "project_id": 654122609,
    "content": "âœ¨ <b>lmdeploy</b> | Python\n<br>\nðŸŽ¯ <b>Primary Use Case:</b>\nCompressing, deploying, and serving large language models (LLMs).\n<br>\nâœ¨ <b>Key Features:</b>\nâ€¢ LLM Compression\nâ€¢ LLM Deployment\nâ€¢ LLM Serving\nâ€¢ PyTorch Inference Engine\nâ€¢ TurboMind Inference Engine\n<br>\nðŸ“– <b>Summary:</b>\nLMDeploy is a toolkit designed to streamline the process of working with large language models. It focuses on compressing models for efficient storage, deploying them across various platforms, and serving them for inference. The toolkit includes features like quantization, a PyTorch inference engine, and support for VLMs.\n<br>\nðŸ”— <b>Links:</b>\nâ€¢ <a href=\"https://github.com/InternLM/lmdeploy?embed=0\">View Project</a>\nâ€¢ <a href=\"https://lmdeploy.readthedocs.io/en/latest/?embed=0\">Homepage</a>\n================\n<a href='https://t.me/GitHub_Open_Source'>ðŸ”“ Open Source</a>",
    "media_url": "https://opengraph.githubassets.com/a1a0de223eaba705e92456a73b4a7cd656a53ad9c6417d31eda7ec4d825398bb/InternLM/lmdeploy",
    "platform": "telegram",
    "quality_score": 0.95
}