{
    "project_id": 659450105,
    "content": "ðŸ”¥ <b>jetson-intro-to-distillation</b> | Python\n<br>\nðŸŽ¯ <b>Primary Use Case:</b>\nOptimizing large models for deployment on NVIDIA Jetson devices using knowledge distillation.\n<br>\nâœ¨ <b>Key Features:</b>\nâ€¢ Knowledge distillation from OpenCLIP to ResNet18\nâ€¢ Classification on the STL10 dataset\nâ€¢ Model profiling and optimization for NVIDIA Jetson\nâ€¢ TensorRT optimization\n<br>\nðŸ“– <b>Summary:</b>\nThis repository provides a tutorial on knowledge distillation, demonstrating how to transfer knowledge from a large OpenCLIP model to a smaller ResNet18 model for image classification on the STL10 dataset. It explores the impact of various factors like data, distillation methods, and model architecture on the final accuracy. The tutorial also covers model optimization using TensorRT for deployment on NVIDIA Jetson devices.\n<br>\nðŸ”— <b>Links:</b>\nâ€¢ <a href=\"https://github.com/NVIDIA-AI-IOT/jetson-intro-to-distillation?embed=0\">View Project</a>\n================\n<a href='https://t.me/GitHub_Open_Source'>ðŸ”“ Open Source</a>",
    "media_url": "https://opengraph.githubassets.com/25323c2744bd56cfb6fa8f795c3740d809b28412ba0e2df7784c885c30f61354/NVIDIA-AI-IOT/jetson-intro-to-distillation",
    "platform": "telegram",
    "quality_score": 0.4
}