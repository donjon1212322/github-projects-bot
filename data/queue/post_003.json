{
    "project_id": 156939672,
    "content": "âœ¨ <b>onnxruntime</b> | C++\n<br>\nðŸŽ¯ <b>Primary Use Case:</b>\nMachine learning model inference and training acceleration\n<br>\nâœ¨ <b>Key Features:</b>\nâ€¢ Cross-platform inference and training accelerator\nâ€¢ Supports models from PyTorch, TensorFlow/Keras, scikit-learn, LightGBM, XGBoost\nâ€¢ Compatible with various hardware, drivers, and operating systems\nâ€¢ Leverages hardware accelerators for optimal performance\nâ€¢ Accelerates model training time on multi-node NVIDIA GPUs for transformer models\n<br>\nðŸ“– <b>Summary:</b>\nONNX Runtime accelerates machine learning model inference and training.  It supports multiple frameworks and hardware, optimizing performance through hardware acceleration and graph optimizations.  Key features include cross-platform compatibility and support for various model types.\n<br>\nðŸ”— <b>Links:</b>\nâ€¢ <a href=\"https://github.com/microsoft/onnxruntime?embed=0\">View Project</a>\nâ€¢ <a href=\"https://onnxruntime.ai?embed=0\">Homepage</a>\n================\n<a href='https://t.me/GitHub_Open_Source'>ðŸ”“ Open Source</a>",
    "media_url": "https://repository-images.githubusercontent.com/156939672/37b1cb00-864b-11eb-8220-76153935f7da",
    "platform": "telegram",
    "quality_score": 0.9500000000000001
}