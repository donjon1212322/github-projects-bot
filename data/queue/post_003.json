{
    "project_id": 607845880,
    "content": "ðŸ’¡ <b>maxtext</b> | Python\n<br>\nðŸŽ¯ <b>Primary Use Case:</b>\nTraining and inference of large language models.\n<br>\nâœ¨ <b>Key Features:</b>\nâ€¢ High performance LLM framework\nâ€¢ Scalable training and inference on TPUs and GPUs\nâ€¢ Written in pure Python/Jax\nâ€¢ Supports Llama 2, Llama 3, Llama 4, Mistral and Mixtral family, Gemma, Gemma 2, Gemma 3, and DeepSeek family models\n<br>\nðŸ“– <b>Summary:</b>\nMaxText is a high-performance, scalable, open-source LLM framework written in pure Python/Jax. It targets Google Cloud TPUs and GPUs for both training and inference. MaxText supports various open models, including Llama, Mistral, Gemma, and DeepSeek families, aiming to provide a launching point for ambitious LLM projects in research and production.\n<br>\nðŸ”— <b>Links:</b>\nâ€¢ <a href=\"https://github.com/AI-Hypercomputer/maxtext?embed=0\">View Project</a>\n================\n<a href='https://t.me/GitHub_Open_Source'>ðŸ”“ Open Source</a>",
    "media_url": "https://opengraph.githubassets.com/567849449221d2ee7cb96116411fa2d5902017d8a13c627b73b7f6a13cec346a/AI-Hypercomputer/maxtext",
    "platform": "telegram",
    "quality_score": 0.85
}