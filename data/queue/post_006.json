{
    "project_id": 996851962,
    "content": "âœ¨ <b>tokasaurus</b> | Python\n<br>\nðŸŽ¯ <b>Primary Use Case:</b>\nHigh-throughput LLM inference\n<br>\nâœ¨ <b>Key Features:</b>\nâ€¢ LLM inference engine\nâ€¢ High-throughput workloads\nâ€¢ OpenAI API support\nâ€¢ Data, pipeline, and tensor parallelism\nâ€¢ Llama3 and Qwen2 architecture support\n<br>\nðŸ“– <b>Summary:</b>\nTokasaurus is an LLM inference engine designed for high-throughput workloads, supporting features like OpenAI APIs, data parallelism, Llama3/Qwen2 architectures, and paged KV caching. It focuses on efficiency with low CPU overhead, CUDA graphs, and a scheduler to maximize batch size while preventing out-of-memory errors and recompiles, making it suitable for deploying LLMs in production environments.\n<br>\nðŸ”— <b>Links:</b>\nâ€¢ <a href=\"https://github.com/ScalingIntelligence/tokasaurus?embed=0\">View Project</a>\n================\n<a href='https://t.me/GitHub_Open_Source'>ðŸ”“ Open Source</a>",
    "media_url": "https://opengraph.githubassets.com/9971826fc7386724ce24976f1c66986d14db31e94e88128001a5ff7a028552c7/ScalingIntelligence/tokasaurus",
    "platform": "telegram",
    "quality_score": 0.85
}