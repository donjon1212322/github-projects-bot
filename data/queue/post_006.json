{
    "project_id": 869660918,
    "content": "ðŸ”¥ <b>mle-bench</b> | Python\n<br>\nðŸŽ¯ <b>Primary Use Case:</b>\nBenchmarking AI agents' performance in machine learning engineering tasks.\n<br>\nâœ¨ <b>Key Features:</b>\nâ€¢ Dataset construction code\nâ€¢ Evaluation logic\nâ€¢ Evaluated agents\nâ€¢ Benchmarking setup recommendations\nâ€¢ Lite evaluation using Low complexity split\n<br>\nðŸ“– <b>Summary:</b>\nMLE-bench is a benchmark designed to evaluate how well AI agents perform in machine learning engineering. It provides code for dataset construction, evaluation logic, and pre-evaluated agents. The repository also offers a recommended benchmarking setup and a 'lite' evaluation option using a low-complexity dataset split for faster and more cost-effective assessments.\n<br>\nðŸ”— <b>Links:</b>\nâ€¢ <a href=\"https://github.com/openai/mle-bench?embed=0\">View Project</a>\nâ€¢ <a href=\"https://openai.com/index/mle-bench/?embed=0\">Homepage</a>\n================\n<a href='https://t.me/GitHub_Open_Source'>ðŸ”“ Open Source</a>",
    "media_url": "https://opengraph.githubassets.com/42ef58cb97aafe10f05810e7f6d71a4e1269f69cfdf06f3899bf06ff83ed3cd9/openai/mle-bench",
    "platform": "telegram",
    "quality_score": 0.7
}