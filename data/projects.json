[
    {
        "id": 1005949992,
        "name": "doggo",
        "description": "üêï your loyal digital companion who finds files the way you think about them.",
        "url": "https://github.com/0nsh/doggo",
        "language": "Python",
        "stars": 23,
        "forks": 1,
        "created_at": "2025-06-21T06:38:26Z",
        "updated_at": "2025-06-26T09:35:45Z",
        "topics": [],
        "quality_score": 0.6499999999999999,
        "contributors_count": 0,
        "last_commit_date": "2025-06-21T07:47:49Z",
        "media_urls": [
            "https://opengraph.githubassets.com/5d215a0447eb83b6e6d0b0615f6f5de2886bc674cf2795d697020593a88aaa9f/0nsh/doggo"
        ],
        "homepage": null,
        "readme_summary": "Doggo is a CLI tool that allows users to search for images using natural language. It leverages AI to understand the content of images and match them to user queries, providing a more intuitive search experience compared to traditional filename-based searches. The tool indexes images using OpenAI's Vision API and stores embeddings in a ChromaDB database for efficient retrieval.",
        "key_features": [
            "Semantic Search: Find images by describing them in natural language",
            "Smart Results: AI-powered similarity matching",
            "CLI Interface: Simple command-line interface",
            "Rich Output: Beautiful, informative search results"
        ],
        "primary_use_case": "Searching for images using natural language queries instead of exact filenames.",
        "open_issues": 5,
        "cover_image_prompt": "A diligent librarian, surrounded by towering shelves of physical image files, uses a magical magnifying glass that transforms spoken queries into perfectly retrieved images. The librarian stands in a grand, sunlit library with digital particles subtly swirling around, highlighting the connection to AI. Small UI elements float nearby, displaying search queries and image previews. The repository name, 'Doggo,' is subtly etched on the magnifying glass. The scene is in a 3D isometric illustration style with warm lighting and rich details, creating a whimsical yet informative atmosphere."
    },
    {
        "id": 942672211,
        "name": "lazyeat",
        "description": "LazyeatÔΩúÊâãÂäøÈöîÁ©∫ÊéßÂà∂ÔºåÊØîÂàíÂ∞±Ë°åÔºÅLazyeat is a touch-free controller for use while eating! Don't want greasy hands while watching shows or browsing the web during meals? You can pause videos/full screen/switch videos just by gesturing to the camera!",
        "url": "https://github.com/lanxiuyun/lazyeat",
        "language": "Vue",
        "stars": 762,
        "forks": 31,
        "created_at": "2025-03-04T13:39:06Z",
        "updated_at": "2025-06-26T09:32:28Z",
        "topics": [
            "accessibility",
            "application",
            "computer-vision",
            "gesture-detection",
            "gesture-recognition",
            "hands-free",
            "mediapipe",
            "mediapipe-hands",
            "multitasking",
            "productivity-tool",
            "python",
            "tauri",
            "tauri-app",
            "vue3",
            "webcam-hacks",
            "windows"
        ],
        "quality_score": 0.9,
        "contributors_count": 0,
        "last_commit_date": "2025-06-16T03:03:53Z",
        "media_urls": [
            "https://opengraph.githubassets.com/5737d5302e532dcf7105269677490d87c42feaf5fd7edd4151567a3ff447342c/lanxiuyun/lazyeat"
        ],
        "homepage": "",
        "readme_summary": "Lazyeat is a hands-free controller that uses gesture recognition to allow users to control their devices while eating. It supports various gestures for cursor control, clicks, scrolling, and key presses, as well as voice input. The primary use case is to avoid touching devices with greasy hands during meals.",
        "key_features": [
            "Hands-free control using gestures",
            "Single finger cursor control",
            "Two-finger click/Rock gesture for mouse click",
            "OK gesture for page scrolling",
            "Four-finger gesture for key press",
            "Voice input support"
        ],
        "primary_use_case": "Touch-free control of devices while eating to avoid getting them greasy.",
        "open_issues": 9,
        "cover_image_prompt": "A person wearing a bib, happily watching a video on a tablet while eating a messy meal. Their hands are clean and free, gesturing towards the screen. The tablet responds to the gestures, pausing and playing the video. Subtle UI elements float around the tablet, showing gesture recognition and control. The scene is set in a cozy dining room with warm lighting. The repository name, 'Lazyeat,' is subtly displayed on the tablet's screen. The image should be in a 3D isometric illustration style with vibrant colors and clear details."
    },
    {
        "id": 684322132,
        "name": "musializer",
        "description": "Music Visualizer",
        "url": "https://github.com/tsoding/musializer",
        "language": "C",
        "stars": 1163,
        "forks": 113,
        "created_at": "2023-08-28T22:50:24Z",
        "updated_at": "2025-06-26T08:56:05Z",
        "topics": [],
        "quality_score": 0.7,
        "contributors_count": 0,
        "last_commit_date": "2025-05-16T01:36:37Z",
        "media_urls": [
            "https://opengraph.githubassets.com/de8ffe3055517159b98e3fa63cffe260508b51d58f8cff973599755a195f694c/tsoding/musializer"
        ],
        "homepage": null,
        "readme_summary": "Musializer is a tool for generating beautiful music visualizations and rendering them into high-quality videos. It supports a variety of audio formats and features a custom build system called `nob`. The project also offers hot reloading capabilities for rapid development and iteration on visualizations.",
        "key_features": [
            "Music visualization",
            "High-quality video rendering",
            "Support for multiple audio formats (wav, ogg, mp3, qoa, xm, mod, flac)",
            "Custom build system (nob)",
            "Hot reloading"
        ],
        "primary_use_case": "Creating music visualizations and rendering high-quality videos of them.",
        "open_issues": 23,
        "cover_image_prompt": "A conductor orchestrating a symphony, where each instrument represents a different audio frequency. The conductor's baton glows with digital light, translating the music into a vibrant, pulsating visual display projected onto a large screen behind the orchestra. Small UI elements float around the screen, showing waveforms and spectrum analyzers. The orchestra is set in a modern concert hall with dynamic lighting. The repository name, 'Musializer', is subtly displayed on the conductor's stand. The image should be in a digital painting style with vivid colors and a sense of movement."
    },
    {
        "id": 969304270,
        "name": "Chrome-OLED-Mode",
        "description": "Chrome OLED Mode adds a pitch black theme to websites, making them high contrast and easy to read at night.",
        "url": "https://github.com/FreelanceProgrammingServices/Chrome-OLED-Mode",
        "language": "JavaScript",
        "stars": 36,
        "forks": 5,
        "created_at": "2025-04-19T21:01:16Z",
        "updated_at": "2025-06-26T09:06:52Z",
        "topics": [],
        "quality_score": 0.65,
        "contributors_count": 0,
        "last_commit_date": "2025-05-16T19:37:42Z",
        "media_urls": [
            "https://opengraph.githubassets.com/fdac8882326bb60e6fa975ac273a6da04d29bc0dd735fc4e9f6699b99dad986b/FreelanceProgrammingServices/Chrome-OLED-Mode"
        ],
        "homepage": null,
        "readme_summary": "The Chrome OLED Mode extension provides a pitch-black theme for websites, enhancing readability and reducing eye strain. It utilizes React for dynamic rendering and offers features like site-specific themes and whitelist management. The extension is designed to be actively maintained and improved, offering an alternative to other dark mode extensions.",
        "key_features": [
            "Pitch black theme for websites",
            "High contrast mode",
            "Site-specific themes",
            "Whitelist management",
            "Automatic scheduling",
            "React-based dynamic rendering",
            "Parcel bundling"
        ],
        "primary_use_case": "Applying a dark, high-contrast theme to websites for comfortable reading, especially at night or on OLED screens.",
        "open_issues": 0,
        "cover_image_prompt": "An ophthalmologist in a futuristic clinic adjusting the settings on a website interface displayed on a large OLED screen, making it easier to read. The room is bathed in a soft, dark ambient light. The website interface dynamically shifts to a high-contrast, pitch-black theme. Small UI elements and code snippets float around the screen, subtly connecting the metaphor to software. The composition is clean and focused, with a blue and purple color scheme. The image should be in 3D isometric illustration style with rich details and vibrant colors."
    },
    {
        "id": 965313467,
        "name": "droidrun",
        "description": null,
        "url": "https://github.com/droidrun/droidrun",
        "language": "Python",
        "stars": 3180,
        "forks": 293,
        "created_at": "2025-04-12T22:03:47Z",
        "updated_at": "2025-06-26T09:54:07Z",
        "topics": [],
        "quality_score": 0.75,
        "contributors_count": 0,
        "last_commit_date": "2025-06-24T15:51:16Z",
        "media_urls": [
            "https://opengraph.githubassets.com/f0e3b110f3935a48bdba6f8c8464bd7192db7b71cf394273a00a997bfaf3e417/droidrun/droidrun"
        ],
        "homepage": "https://droidrun.ai",
        "readme_summary": "DroidRun is a framework for controlling Android and iOS devices using LLM agents, enabling automation of device interactions through natural language commands. It supports multiple LLM providers, offers planning capabilities for complex tasks, and provides an extendable Python API for custom automations. The framework also includes screenshot analysis and execution tracing for enhanced debugging.",
        "key_features": [
            "Control Android and iOS devices with natural language commands",
            "Supports multiple LLM providers",
            "Planning capabilities for complex multi-step tasks",
            "Easy to use CLI with enhanced debugging features",
            "Extendable Python API for custom automations",
            "Screenshot analysis for visual understanding",
            "Execution tracing"
        ],
        "primary_use_case": "Automating device interactions using natural language commands.",
        "open_issues": 9,
        "cover_image_prompt": "Imagine a skilled conductor orchestrating a symphony of smartphones and tablets. Each device plays a unique tune, responding harmoniously to the conductor's baton, which is subtly shaped like a Python script. The devices float in a digital space filled with glowing particles representing data streams. Small UI elements appear around the devices, showcasing natural language commands being translated into device actions. The scene is bathed in soft, ethereal light, creating a sense of seamless control and automation. The image should be in a 3D isometric illustration style with vibrant colors and intricate details."
    },
    {
        "id": 247266215,
        "name": "EasyOCR",
        "description": "Ready-to-use OCR with 80+ supported languages and all popular writing scripts including Latin, Chinese, Arabic, Devanagari, Cyrillic and etc.",
        "url": "https://github.com/JaidedAI/EasyOCR",
        "language": "Python",
        "stars": 27053,
        "forks": 3373,
        "created_at": "2020-03-14T11:46:39Z",
        "updated_at": "2025-06-26T09:48:41Z",
        "topics": [
            "cnn",
            "crnn",
            "data-mining",
            "deep-learning",
            "easyocr",
            "image-processing",
            "information-retrieval",
            "lstm",
            "machine-learning",
            "ocr",
            "optical-character-recognition",
            "python",
            "pytorch",
            "scene-text",
            "scene-text-recognition"
        ],
        "quality_score": 0.6000000000000001,
        "contributors_count": 0,
        "last_commit_date": "2024-09-24T11:18:06Z",
        "media_urls": [
            "https://opengraph.githubassets.com/39c5469765a29a6f49f3b3254a248deb7012dd94ac560b6f3b018fee74fd32e7/JaidedAI/EasyOCR"
        ],
        "homepage": "https://www.jaided.ai",
        "readme_summary": "EasyOCR is a Python library for performing OCR, supporting over 80 languages and various writing scripts. It offers a simple installation process and can extract text from various image formats, including files, OpenCV objects, and URLs, making it easy to integrate into different applications.",
        "key_features": [
            "Supports 80+ languages",
            "Supports multiple writing scripts",
            "Easy installation via pip",
            "Can read text from images, OpenCV objects, bytes, and URLs"
        ],
        "primary_use_case": "Optical Character Recognition (OCR) for extracting text from images in multiple languages.",
        "open_issues": 457,
        "cover_image_prompt": "Imagine a skilled librarian effortlessly retrieving information from stacks of books written in various languages (Chinese, Arabic, Latin, etc.). The librarian uses a special monocle that instantly translates the text into a readable format, displaying the translated text as glowing overlays on the books. Subtle UI elements float around the librarian, showing code snippets and language detection probabilities. The scene is set in a brightly lit, futuristic library with a clean, organized aesthetic. The image should be in a 3D isometric illustration style with vibrant colors and sharp details, subtly incorporating the EasyOCR logo."
    }
]